{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint 14 Keras",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yellan-Kargbo/Assignment1/blob/main/Sprint_14_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGmwHp-iEfC"
      },
      "source": [
        "# solution 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Ptm0MiOuni"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, BatchNormalization, Activation, GlobalAveragePooling1D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMlNMUJjM0Ww"
      },
      "source": [
        "# Sharing and executing the official tutorial model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "mwGrCNk9PF5N",
        "outputId": "d3d0b9c5-0193-450f-8ec6-7451b8bd344e"
      },
      "source": [
        "# Movie review dataset loading\n",
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3193a31451b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Movie review dataset loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras' has no attribute 'datasets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBTCI9tqPIN-"
      },
      "source": [
        "train_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq8VvndvPZcr"
      },
      "source": [
        "# The first part of the index is reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "# Create a reverse dictionary\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Create a function for reverse lookup\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB46e32GPf1Q"
      },
      "source": [
        "decode_review(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_s3dv3OPjtr"
      },
      "source": [
        "print(len(train_data[0]))\n",
        "print(len(train_data[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obds4RXxPmle"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "train_data = sequence.pad_sequences(train_data,\n",
        "                                    value=word_index[\"<PAD>\"],\n",
        "                                    padding=\"post\",\n",
        "                                    maxlen=256)\n",
        "\n",
        "test_data = sequence.pad_sequences(test_data,\n",
        "                                    value=word_index[\"<PAD>\"],\n",
        "                                    padding=\"post\",\n",
        "                                    maxlen=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mydGXVnEPo7O"
      },
      "source": [
        "print(len(train_data[0]))\n",
        "print(len(train_data[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPayBzfpPrM9"
      },
      "source": [
        "embedding_dim=16\n",
        "vocab_size = 10000\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c107QVeePuEt"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Nn-UiRPwhI"
      },
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = EarlyStopping(patience=3)\n",
        "history = model.fit(train_data, train_labels, batch_size=512 ,epochs=100, callbacks=callbacks ,validation_split=0.2)\n",
        "model.evaluate(test_data, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_XZHp2oPzkX"
      },
      "source": [
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
        "\n",
        "axes[0].plot(epochs, loss, 'bo', label='Training loss')\n",
        "axes[0].plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "axes[0].set_title('Training and validation loss')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(epochs, acc, 'bo', label='Training acc')\n",
        "axes[1].plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "axes[1].set_title('Training and validation accuracy')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend(loc='lower right')\n",
        "axes[1].set_ylim((0.5,1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Bwuaj4P3At"
      },
      "source": [
        "# Get the weight of the embedding layer\n",
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) ## shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aayvX8peP9SM"
      },
      "source": [
        "#[Problem 2] (Advance assignment) Execute various methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_HAuT4qQBua"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import special_ortho_group\n",
        "from scipy.spatial.transform import Rotation\n",
        "from scipy.linalg import svd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "FIGURE_SCALE = 1.0\n",
        "FONT_SIZE = 20\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': np.array((8, 6)) * FIGURE_SCALE,\n",
        "    'axes.labelsize': FONT_SIZE,\n",
        "    'axes.titlesize': FONT_SIZE,\n",
        "    'xtick.labelsize': FONT_SIZE,\n",
        "    'ytick.labelsize': FONT_SIZE,\n",
        "    'legend.fontsize': FONT_SIZE,\n",
        "    'lines.linewidth': 3,\n",
        "    'lines.markersize': 10,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrqa6vu5QES4"
      },
      "source": [
        "def SO3_via_svd(A):\n",
        "  \"\"\"Map 3x3 matrix onto SO(3) via SVD.\"\"\"\n",
        "  u, s, vt = np.linalg.svd(A)\n",
        "  s_SO3 = [1, 1, np.sign(np.linalg.det(np.matmul(u, vt)))]\n",
        "  return np.matmul(np.matmul(u, np.diag(s_SO3)), vt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWUSl-XvQGff"
      },
      "source": [
        "def SO3_via_gramschmidt(A):\n",
        "  \"\"\"Map 3x3 matrix on SO(3) via GS, ignores last column.\"\"\"\n",
        "  x_normalized = A[:, 0] / np.linalg.norm(A[:, 0])\n",
        "  z = np.cross(x_normalized, A[:, 1])\n",
        "  z_normalized = z / np.linalg.norm(z)\n",
        "  y_normalized = np.cross(z_normalized, x_normalized)\n",
        "  return np.stack([x_normalized, y_normalized, z_normalized], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruB4LBKlQLZS"
      },
      "source": [
        "def rotate_from_z(v):\n",
        "  \"\"\"Construct a rotation matrix R such that R * [0,0,||v||]^T = v.\n",
        "\n",
        "  Input v is shape (3,), output shape is 3x3 \"\"\"\n",
        "  vn = v / np.linalg.norm(v)\n",
        "  theta = np.arccos(vn[2])\n",
        "  phi = np.arctan2(vn[1], vn[0])\n",
        "  r = Rotation.from_euler('zyz', [0, theta, phi])\n",
        "  R = np.squeeze(r.as_dcm()) # Maps Z to vn\n",
        "  return R\n",
        "\n",
        "def perturb_rotation_matrix(R, kappa):\n",
        "  \"\"\"Perturb a random rotation matrix with noise.\n",
        "\n",
        "  Noise is random small rotation applied to each of the three\n",
        "  column vectors of R. Angle of rotation is sampled from the\n",
        "  von-Mises distribution on the circle (with uniform random azimuth).\n",
        "\n",
        "  The von-Mises distribution is analagous to Gaussian distribution on the circle.\n",
        "  Note, the concentration parameter kappa is inversely related to variance,\n",
        "  so higher kappa means less variance, less noise applied. Good ranges for\n",
        "  kappa are 64 (high noise) up to 512 (low noise).\n",
        "  \"\"\"\n",
        "  R_perturb = []\n",
        "  theta = np.random.vonmises(mu=0.0, kappa=kappa, size=(3,))\n",
        "  phi = np.random.uniform(low=0.0, high=np.pi*2.0, size=(3,))\n",
        "  for i in range(3):\n",
        "    v = R[:, i]\n",
        "    R_z_to_v = rotate_from_z(v)\n",
        "    r_noise_z = np.squeeze(Rotation.from_euler('zyz', [0, theta[i], phi[i]]).as_dcm())\n",
        "\n",
        "    v_perturb = np.matmul(R_z_to_v, np.matmul(r_noise_z, np.array([0,0,1])))\n",
        "    R_perturb.append(v_perturb)\n",
        "\n",
        "  R_perturb = np.stack(R_perturb, axis=-1)\n",
        "  return R_perturb\n",
        "\n",
        "\n",
        "def sigma_to_kappa(sigma):\n",
        "  return ((0.5 - sigma) * 1024) + 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHk-XdzmQeUD"
      },
      "source": [
        "# We create a ground truth special orthogonal matrix and perturb it with\n",
        "# additive noise. We then see which orthogonalization process (SVD or GS) is\n",
        "# better at recovering the ground truth matrix.\n",
        "\n",
        "\n",
        "def run_expt(sigmas, num_trials, noise_type='gaussian'):\n",
        "  # Always use identity as ground truth, or pick random matrix.\n",
        "  # Nothing should change if we pick random (can verify by setting to True) since\n",
        "  # SVD and Gram-Schmidt are both Equivariant to rotations.\n",
        "  pick_random_ground_truth=False\n",
        "\n",
        "  all_errs_svd = []\n",
        "  all_errs_gs = []\n",
        "  all_geo_errs_svd = []\n",
        "  all_geo_errs_gs = []\n",
        "  all_noise_norms = []\n",
        "  all_noise_sq_norms = []\n",
        "\n",
        "  for sig in sigmas:\n",
        "    svd_errors = np.zeros(num_trials)\n",
        "    gs_errors = np.zeros(num_trials)\n",
        "    svd_geo_errors = np.zeros(num_trials)\n",
        "    gs_geo_errors = np.zeros(num_trials)\n",
        "    noise_norms = np.zeros(num_trials)\n",
        "    noise_sq_norms = np.zeros(num_trials)\n",
        "\n",
        "    for t in range(num_trials):\n",
        "      if pick_random_ground_truth:\n",
        "        A = special_ortho_group.rvs(3)  # Pick a random ground truth matrix\n",
        "      else:\n",
        "        A = np.eye(3)  # Our ground truth matrix in SO(3)\n",
        "\n",
        "      N = None\n",
        "      if noise_type == 'gaussian':\n",
        "        N = np.random.standard_normal(size=(3,3)) * sig\n",
        "      if noise_type == 'uniform':\n",
        "        N = np.random.uniform(-1, 1, (3, 3)) * sig\n",
        "      if noise_type == 'rademacher':\n",
        "        N = np.sign(np.random.uniform(-1, 1, (3, 3))) * sig\n",
        "      if noise_type == 'rotation':\n",
        "        A_perturb = perturb_rotation_matrix(A, kappa=sigma_to_kappa(sig))\n",
        "        N = A_perturb - A\n",
        "      if N is None:\n",
        "        print ('Error: unknown noise_type: %s', noise_type)\n",
        "        return\n",
        "\n",
        "      AplusN = A + N  # Ground-truth plus noise\n",
        "      noise_norm = np.linalg.norm(N)\n",
        "      noise_norm_sq = noise_norm**2\n",
        "\n",
        "      # Compute SVD result and error.\n",
        "      res_svd = SO3_via_svd(AplusN)\n",
        "      error_svd = np.linalg.norm(res_svd - A, ord='fro')**2\n",
        "      error_geodesic_svd = np.arccos(\n",
        "          (np.trace(np.matmul(np.transpose(res_svd), A))-1.0)/2.0);\n",
        "\n",
        "      # Compute GS result and error.\n",
        "      res_gs = SO3_via_gramschmidt(AplusN)\n",
        "      error_gs = np.linalg.norm(res_gs - A, ord='fro')**2\n",
        "      error_geodesic_gs = np.arccos(\n",
        "          (np.trace(np.matmul(np.transpose(res_gs), A))-1.0)/2.0);\n",
        "\n",
        "      svd_errors[t] = error_svd\n",
        "      gs_errors[t] = error_gs\n",
        "      svd_geo_errors[t] = error_geodesic_svd\n",
        "      gs_geo_errors[t] = error_geodesic_gs\n",
        "      noise_norms[t] = noise_norm\n",
        "      noise_sq_norms[t] = noise_norm_sq\n",
        "\n",
        "    all_errs_svd.append(svd_errors)\n",
        "    all_errs_gs.append(gs_errors)\n",
        "    all_geo_errs_svd.append(svd_geo_errors)\n",
        "    all_geo_errs_gs.append(gs_geo_errors)\n",
        "    all_noise_norms.append(noise_norms)\n",
        "    all_noise_sq_norms.append(noise_sq_norms)\n",
        "    print('finished sigma = %f / kappa = %f' % (sig, sigma_to_kappa(sig)))\n",
        "\n",
        "  return [np.array(x) for x in (\n",
        "      all_errs_svd, all_errs_gs,\n",
        "      all_geo_errs_svd, all_geo_errs_gs,\n",
        "      all_noise_norms, all_noise_sq_norms)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHG2xT4mQhMx"
      },
      "source": [
        "boxprops = dict(linewidth=2)\n",
        "medianprops = dict(linewidth=2)\n",
        "whiskerprops = dict(linewidth=2)\n",
        "capprops = dict(linewidth=2)\n",
        "\n",
        "def make_diff_plot(svd_errs, gs_errs, xvalues, title='', ytitle='', xtitle=''):\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.title(title, fontsize=16)\n",
        "  diff = gs_errs - svd_errs\n",
        "  step_size = np.abs(xvalues[1] - xvalues[0])\n",
        "  plt.boxplot(diff.T, positions=xvalues, widths=step_size/2, whis=[5, 95],\n",
        "              boxprops=boxprops, medianprops=medianprops, whiskerprops=whiskerprops, capprops=capprops,\n",
        "              showmeans=False, meanline=True, showfliers=False)\n",
        "  plt.plot(xvalues, np.max(diff, axis=1), 'kx', markeredgewidth=2)\n",
        "  plt.plot(xvalues, np.min(diff, axis=1), 'kx', markeredgewidth=2)\n",
        "  xlim = [np.min(xvalues) - (step_size / 3), np.max(xvalues) + (step_size / 3)]\n",
        "  plt.xlim(xlim)\n",
        "  plt.plot(xlim, [0, 0], 'k--', linewidth=1)\n",
        "  plt.xlabel(xtitle, fontsize=16)\n",
        "  plt.ylabel(ytitle, fontsize=16)\n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79JmhnbVQlPr"
      },
      "source": [
        "###Global Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX6ak3_7QrE0"
      },
      "source": [
        "num_trials = 100000  # Num trials at each sigma\n",
        "sigmas = np.linspace(0.125, 0.5, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvp6Lcl2QtRc"
      },
      "source": [
        "###Gaussian Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrmGH3IRQxHe"
      },
      "source": [
        "(all_errs_svd, all_errs_gs,\n",
        " all_geo_errs_svd, all_geo_errs_gs,\n",
        " all_noise_norms, all_noise_sq_norms\n",
        " ) = run_expt(sigmas, num_trials, noise_type='gaussian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ3zl9q8Q0La"
      },
      "source": [
        "plt.plot(sigmas,\n",
        "         3*sigmas**2,\n",
        "         '--b',\n",
        "         label='3 $\\\\sigma^2$')\n",
        "plt.errorbar(sigmas,\n",
        "             all_errs_svd.mean(axis=1),\n",
        "             color='b',\n",
        "             label='E[$\\\\|\\\\|\\\\mathrm{SVD}^+(M) - R\\\\|\\\\|_F^2]$')\n",
        "\n",
        "plt.plot(sigmas, 6*sigmas**2,\n",
        "         '--r',\n",
        "         label='6 $\\\\sigma^2$')\n",
        "plt.errorbar(sigmas,\n",
        "             all_errs_gs.mean(axis=1),\n",
        "             color='r',\n",
        "             label='E[$\\\\|\\\\|\\\\mathrm{GS}^+(M) - R\\\\|\\\\|_F^2$]')\n",
        "\n",
        "plt.xlabel('$\\\\sigma$')\n",
        "plt.legend(loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1EJuqO9Q4R4"
      },
      "source": [
        "make_diff_plot(all_errs_svd, all_errs_gs, sigmas, title='Gaussian Noise', ytitle='Frobenius Error Diff', xtitle='$\\\\sigma$')\n",
        "make_diff_plot(all_geo_errs_svd, all_geo_errs_gs, sigmas, title='Gaussian Noise', ytitle='Geodesic Error Diff', xtitle='$\\\\sigma$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RCtzWMMRD67"
      },
      "source": [
        "(all_errs_svd, all_errs_gs,\n",
        " all_geo_errs_svd, all_geo_errs_gs,\n",
        " all_noise_norms, all_noise_sq_norms\n",
        " ) = run_expt(sigmas, num_trials, noise_type='uniform')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYZbjkVMRGhF"
      },
      "source": [
        "make_diff_plot(all_errs_svd, all_errs_gs, sigmas, title='Uniform Noise', ytitle='Frobenius Error Diff', xtitle='$\\\\phi$')\n",
        "make_diff_plot(all_geo_errs_svd, all_geo_errs_gs, sigmas, title='Uniform Noise', ytitle='Geodesic Error Diff', xtitle='$\\\\phi$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyhpzFYBRTB9"
      },
      "source": [
        "# solution 3 Learning Iris (binary classification) with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "734G1XXCRWWi"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[50:]\n",
        "y = iris.target[50:]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5kWHhw5RYjb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "sc = StandardScaler().fit(X_train)\n",
        "X_train = sc.transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "le = LabelBinarizer()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "y_test = le.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoQeiZbcRb8k"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def iris_nn_classifier(n_features):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, activation=\"relu\", input_shape=(n_features,)))\n",
        "  model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "n_features = 4\n",
        "model = iris_nn_classifier(n_features)\n",
        "history = model.fit(X_train, y_train, batch_size=20, epochs=10, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh8vAOuJRqXZ"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGQH36UnRz9o"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "sc = StandardScaler().fit(X_train)\n",
        "X_train = sc.transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ36T2uTR37m"
      },
      "source": [
        "def iris_nn_multi_classifier(n_features):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, activation=\"relu\", input_shape=(n_features,)))\n",
        "  model.add(Dense(128, activation=\"relu\"))\n",
        "  model.add(Dense(64, activation=\"relu\"))\n",
        "  model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "n_features = 4\n",
        "model = iris_nn_multi_classifier(n_features)\n",
        "history = model.fit(X_train, y_train, batch_size=20, epochs=10, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp_7EQVbR7x7"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjJWIBQWR-0a"
      },
      "source": [
        "# Learning House Prices with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHiU7UHrSCDs"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "X_train = df.loc[:, [\"TotalBsmtSF\", \"YearBuilt\", \"GarageArea\"]]\n",
        "y_train = df.loc[:, \"SalePrice\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=0)\n",
        "\n",
        "sc = StandardScaler().fit(X_train)\n",
        "X_train = sc.transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwEa2FrlSFbz"
      },
      "source": [
        "def nn_regression(n_features, n_output):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, activation=\"relu\", input_shape=(n_features,)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(128, activation=\"relu\"))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(64, activation=\"relu\"))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(n_output, activation=\"linear\"))\n",
        "\n",
        "  model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "  return model\n",
        "\n",
        "callbacks = EarlyStopping(patience=3)\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "n_output = 1\n",
        "model = nn_regression(n_features, n_output)\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=10, callbacks=callbacks, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHSOj0CQSIiP"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZAnwTx4SNN-"
      },
      "source": [
        "# Learning MNIST with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoXFiTfoSQyL"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMaaf-fWSTYM"
      },
      "source": [
        "img_height = 28\n",
        "img_width = 28\n",
        "num_features = int(img_height * img_width)\n",
        "\n",
        "X_train = X_train.reshape(-1, num_features).astype(\"float\")\n",
        "X_test = X_test.reshape(-1, num_features).astype(\"float\")\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = to_categorical(y_train.reshape(-1,1))\n",
        "y_test = to_categorical(y_test.reshape(-1,1))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEPBrjFjSWU6"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def mnist_classifier(n_features, n_output):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, activation=\"relu\", input_shape=(n_features,)))\n",
        "  model.add(Dense(128, activation=\"relu\"))\n",
        "  model.add(Dense(64, activation=\"relu\"))\n",
        "  model.add(Dense(n_output, activation=\"softmax\"))\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "callbacks = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=2)\n",
        "\n",
        "model = mnist_classifier(784, 10)\n",
        "history = model.fit(X_train, y_train, batch_size=20, epochs=10, callbacks=callbacks, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WSCSjZNSZD2"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4DvdC-TSgS8"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muiL-mUXSirW"
      },
      "source": [
        "#Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "#Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcTvpNIuSl0W"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbnmJ-ruSstJ"
      },
      "source": [
        "#Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "#Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQpByZchSzSu"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WGcgLlGTFUX"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUua4PJNTIgt"
      },
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfGbHX31TLRR"
      },
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVOZZ8XPTPUC"
      },
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVfaLbRVuZLo"
      },
      "source": [
        "###PyTorch Convolutional Neural Network With MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD4DTD4iudsN"
      },
      "source": [
        "# Import libraries\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8-XIhSwugYB"
      },
      "source": [
        "#Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz0VwWofunLF"
      },
      "source": [
        "#Download MNIST dataset in local system\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,                         \n",
        "    transform = ToTensor(), \n",
        "    download = True,            \n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data', \n",
        "    train = False, \n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehflcWAYuuQP"
      },
      "source": [
        "#Print train_data and test_data size\n",
        "print(train_data)\n",
        "print(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH4xhI85ux6T"
      },
      "source": [
        "print(train_data.data.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxmy8Yyqu0Zk"
      },
      "source": [
        "#Visualization of MNIST dataset\n",
        "#Plot one train_data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_data.data[0], cmap='gray')\n",
        "plt.title('%i' % train_data.targets[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50y3gi6mu5A2"
      },
      "source": [
        "#Plot multiple train_data\n",
        "figure = plt.figure(figsize=(10, 8))\n",
        "cols, rows = 5, 5\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
        "    img, label = train_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(label)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYJwkCDju82p"
      },
      "source": [
        "#Preparing data for training with DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "loaders = {\n",
        "    'train' : torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=100, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=1),\n",
        "    \n",
        "    'test'  : torch.utils.data.DataLoader(test_data, \n",
        "                                          batch_size=100, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=1),\n",
        "}\n",
        "loaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSO52KJbvIh9"
      },
      "source": [
        "import torch.nn as nn\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output, x    # return x for visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B59JMHOxvLcO"
      },
      "source": [
        "cnn = CNN()\n",
        "print(cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSetsCEXvOeh"
      },
      "source": [
        "#Define loss function\n",
        "loss_func = nn.CrossEntropyLoss()   \n",
        "loss_func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6R15qKcvSPA"
      },
      "source": [
        "#Define a Optimization Function\n",
        "from torch import optim\n",
        "optimizer = optim.Adam(cnn.parameters(), lr = 0.01)   \n",
        "optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SEsakBFvXIx"
      },
      "source": [
        "#Train the model\n",
        "from torch.autograd import Variable\n",
        "num_epochs = 10\n",
        "def train(num_epochs, cnn, loaders):\n",
        "    \n",
        "    cnn.train()\n",
        "        \n",
        "    #Train the model\n",
        "    total_step = len(loaders['train'])\n",
        "        \n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(loaders['train']):\n",
        "            \n",
        "            #gives batch data, normalize x when iterate train_loader\n",
        "            b_x = Variable(images)   # batch x\n",
        "            b_y = Variable(labels)   # batch y\n",
        "output = cnn(b_x)[0]               \n",
        "            loss = loss_func(output, b_y)\n",
        "            \n",
        "            #clear gradients for this training step   \n",
        "            optimizer.zero_grad()           \n",
        "            \n",
        "            #backpropagation, compute gradients \n",
        "            loss.backward()    \n",
        "            #apply gradients             \n",
        "            optimizer.step()                \n",
        "            \n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "               pass\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    \n",
        "    pass\n",
        "train(num_epochs, cnn, loaders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8halLjNAvgTz"
      },
      "source": [
        "#Evaluate the model on test data\n",
        "def test():\n",
        "    # Test the model\n",
        "    cnn.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in loaders['test']:\n",
        "            test_output, last_layer = cnn(images)\n",
        "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
        "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
        "            pass\n",
        "print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
        "    \n",
        "    pass\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai59m-8mvj9G"
      },
      "source": [
        "sample = next(iter(loaders['test']))\n",
        "imgs, lbls = sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGHPakvAvmPH"
      },
      "source": [
        "actual_number = lbls[:10].numpy()\n",
        "actual_number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2IgVB3bvo9h"
      },
      "source": [
        "test_output, last_layer = cnn(imgs[:10])\n",
        "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
        "print(f'Prediction number: {pred_y}')\n",
        "print(f'Actual number: {actual_number}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rTlJYr0vt7x"
      },
      "source": [
        "# Predict house prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SSbDflgvx-Z"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxAgqSdyv2bZ"
      },
      "source": [
        "pd.read_csv('california_housing_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_ITosQv5WR"
      },
      "source": [
        "#getting std and mean of training data, as it needs for normalization and denormalization of data\n",
        "\n",
        "train_csv = pd.read_csv('california_housing_train.csv') \n",
        "train_mean =  train_csv.mean()\n",
        "train_std = train_csv.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9GyKesIv80I"
      },
      "source": [
        "#creating custom dataset class\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.norm_data = (self.data_frame - train_mean)/train_std  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.norm_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.norm_data.iloc[idx, 2:8].values # keep all except median_house_value  \n",
        "        label = self.norm_data.iloc[idx, 8:9].values # keep only median_house_value  \n",
        "        \n",
        "        data = torch.tensor(data, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return {'data': data, 'label':label}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtr9jcagwBN4"
      },
      "source": [
        "#Loading our dataset object in DataLoader class\n",
        "train_data = MyDataset('california_housing_train.csv')\n",
        "dataset_len = len(train_data)\n",
        "train_data = torch.utils.data.DataLoader(dataset=train_data, shuffle=True, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-EpIPjywEUj"
      },
      "source": [
        "#Create artificial neural network model class\n",
        "#our model\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=6, out_features=18, bias=True)\n",
        "        self.fc2 = nn.Linear(in_features=18, out_features=18, bias=True)\n",
        "        self.fc3 = nn.Linear(in_features=18, out_features=12, bias=True)\n",
        "        self.fc4 = nn.Linear(in_features=12, out_features=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "net = Network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKPm7Kd4wJNK"
      },
      "source": [
        "#Define optimization and loss functions\n",
        "#loss and optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CLnu28AwMgV"
      },
      "source": [
        "#Create training loop\n",
        "#training the model\n",
        "\n",
        "for epoch in range(15):\n",
        "    running_loss = 0.0\n",
        "    for i, value in enumerate(train_data):\n",
        "        inputs = value['data']\n",
        "        labels = value['label']\n",
        "        prediction = net(inputs) # passing inputs to our model to get prediction\n",
        "        loss = criterion(prediction, labels)\n",
        "        running_loss += loss.item() * inputs.size(0) # multiplying with batch size\n",
        "        optimizer.zero_grad() # reset all gradient calculation\n",
        "        loss.backward() # this is backpropagation to calculate gradients\n",
        "        optimizer.step() # applying gradient descent to update weights and bias values\n",
        "\n",
        "    print('epoch: ', epoch, ' loss: ', running_loss/dataset_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2SSmw8mwQlL"
      },
      "source": [
        "#test the model\n",
        "\n",
        "test_data = MyDataset('california_housing_test.csv')\n",
        "test_data_len = len(test_data)\n",
        "test_dataset = torch.utils.data.DataLoader(dataset=test_data, shuffle=False, batch_size=10)\n",
        "\n",
        "running_loss = 0.0\n",
        "accuracy = 0.0\n",
        "for i, value in enumerate(test_dataset):\n",
        "    inputs = value['data']\n",
        "    labels = value['label']\n",
        "\n",
        "    prediction = net(inputs)\n",
        "    loss = criterion(prediction, labels)\n",
        "\n",
        "        \n",
        "    running_loss += loss.item() * inputs.size(0) # multiplying by batch size\n",
        "\n",
        "print('Test loss: ', running_loss/test_data_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdzhzOtxwT8W"
      },
      "source": [
        "#let's see first 10 predictions\n",
        "\n",
        "test_dataset_sample = next(iter(test_dataset)) # return first batch of data\n",
        "outputs = net(test_dataset_sample['data']) \n",
        "outputs = (outputs * train_std.values[-1]) + train_mean.values[-1] # denormalizing data to see real prices\n",
        "print(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0MpJsgTwXvK"
      },
      "source": [
        "pd.read_csv('california_housing_test.csv').head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGFyaYd0wa6h"
      },
      "source": [
        "# Classifying the Iris Data Set with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXyvOhPzwk-B"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJksBaYwwnxt"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "names = iris['target_names']\n",
        "feature_names = iris['feature_names']\n",
        "\n",
        "# Scale data to have mean 0 and variance 1 \n",
        "# which is importance for convergence of the neural network\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEI2vTApwqqY"
      },
      "source": [
        "## Visualize the Data\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "for target, target_name in enumerate(names):\n",
        "    X_plot = X[y == target]\n",
        "    ax1.plot(X_plot[:, 0], X_plot[:, 1], \n",
        "             linestyle='none', \n",
        "             marker='o', \n",
        "             label=target_name)\n",
        "ax1.set_xlabel(feature_names[0])\n",
        "ax1.set_ylabel(feature_names[1])\n",
        "ax1.axis('equal')\n",
        "ax1.legend();\n",
        "\n",
        "for target, target_name in enumerate(names):\n",
        "    X_plot = X[y == target]\n",
        "    ax2.plot(X_plot[:, 2], X_plot[:, 3], \n",
        "             linestyle='none', \n",
        "             marker='o', \n",
        "             label=target_name)\n",
        "ax2.set_xlabel(feature_names[2])\n",
        "ax2.set_ylabel(feature_names[3])\n",
        "ax2.axis('equal')\n",
        "ax2.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG22frUiwuNc"
      },
      "source": [
        "## Configure Neural Network Models\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFj1d_a6ww4W"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 50)\n",
        "        self.layer2 = nn.Linear(50, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKaUfLvQwzcu"
      },
      "source": [
        "model     = Model(X_train.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn   = nn.CrossEntropyLoss()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM8yyVvrw2Jn"
      },
      "source": [
        "## Train the Model\n",
        "import tqdm\n",
        "\n",
        "EPOCHS  = 100\n",
        "X_train = Variable(torch.from_numpy(X_train)).float()\n",
        "y_train = Variable(torch.from_numpy(y_train)).long()\n",
        "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
        "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
        "\n",
        "loss_list     = np.zeros((EPOCHS,))\n",
        "accuracy_list = np.zeros((EPOCHS,))\n",
        "\n",
        "for epoch in tqdm.trange(EPOCHS):\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss_list[epoch] = loss.item()\n",
        "    \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "        accuracy_list[epoch] = correct.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qdC0amLw6yj"
      },
      "source": [
        "## Plot Accuracy and Loss from Training\n",
        "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "ax1.plot(accuracy_list)\n",
        "ax1.set_ylabel(\"validation accuracy\")\n",
        "ax2.plot(loss_list)\n",
        "ax2.set_ylabel(\"validation loss\")\n",
        "ax2.set_xlabel(\"epochs\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDY-OIr4w-IP"
      },
      "source": [
        "## Show ROC Curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "# One hot encoding\n",
        "enc = OneHotEncoder()\n",
        "Y_onehot = enc.fit_transform(y_test[:, np.newaxis]).toarray()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test).numpy()\n",
        "    fpr, tpr, threshold = roc_curve(Y_onehot.ravel(), y_pred.ravel())\n",
        "    \n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc(fpr, tpr)))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}